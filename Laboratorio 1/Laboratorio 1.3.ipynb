{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Laboratorio 1.1\n",
    "## Calcolare la similarità nelle definizioni dei 4 concetti del dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69d051cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m IPython notebook\n",
    "from nltk import word_tokenize, SnowballStemmer\n",
    "from nltk.corpus import stopwords, words\n",
    "import string\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "from operator import itemgetter\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# Crea la variabile stop_words con le stop word e con la punteggiatura\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(set(stopwords.words('italian')))\n",
    "stop_words.update(set(string.punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098597a",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "## 1. Definizione della funzione per l'estrazione delle informazioni dalla base di dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6390192",
   "metadata": {},
   "outputs": [],
   "source": [
    "media = {}           # Dictionary che conterrà la media aritmetica della lunghezza di ogni definizione per ogni concetto\n",
    "\n",
    "def extract_data(path='dataset/defs.csv'):\n",
    "    if path == 'dataset/defs.csv':\n",
    "        df = pd.read_csv(path, header=0)\n",
    "        df = df.dropna()\n",
    "        df.drop(['Partecipante'],axis=1,inplace=True)\n",
    "        data = df.to_dict()\n",
    "\n",
    "    elif 'dataset/db.csv':\n",
    "        df = pd.read_csv(path, header=0, sep=',')\n",
    "        data = df.values.tolist()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Definizione della funzione per la pulitura dell'input\n",
    "La funzione restituirà un dictionary innestato di questo tipo `{Concetto: {utente 0: [definizione tokenizzata e stemmatizzata]}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sort_indexes(data):\n",
    "    defs = {}\n",
    "\n",
    "    for concept in data.keys():\n",
    "        i = 0\n",
    "        defs.setdefault(concept,{})\n",
    "        for index in data[concept]:\n",
    "            defs[concept].setdefault(i, data[concept][index])\n",
    "            i += 1\n",
    "    return defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "    \n",
    "    if type(data) == list:\n",
    "        for element in range(len(data)):\n",
    "            token = word_tokenize(data[element][0].lower())\n",
    "            token = snow_stemmer.stem(token[0])\n",
    "            del data[element]\n",
    "            data.append(token)\n",
    "\n",
    "    elif type(data) == dict:\n",
    "\n",
    "        for names in data.keys():\n",
    "            for index in data[names]:\n",
    "                data[names][index] = word_tokenize(data[names][index].lower())\n",
    "                data[names][index] = list(set([snow_stemmer.stem(word) for word in data[names][index] if word not in stop_words or word != \"'s\"]) - stop_words)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Esecuzione delle funzioni dichiarate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = extract_data()\n",
    "\n",
    "# Estrazione dei dati dei concetti in un dictionary\n",
    "data = sort_indexes(data)\n",
    "\n",
    "# Pre-processing dell'input\n",
    "data = pre_processing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Laboratorio 1.3\n",
    "## Word Sense Induction\n",
    "Si contrappone al Word Sense Disambiguation che è un problema computazionale aperto in cui si deve riconoscere il **senso** di una parola in una frase, siccome una parola può essere utilizzata con accezioni differenti.\n",
    "Il ***Word Sense Induction*** è invece basato su un meccanismo dove si uniscono due parole (creando una pseudo-word) e si da uno score se quando si itera sul corpus si incontra una delle componenti della pseudoword. Lo score può essere positivo se si ci riferisce a una componente e negativo se ci si riferisce alla seconda componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Restituisce una lista con tutti i token distinte utilizzate\n",
    "def get_tokens(corpus): \n",
    "    words = []\n",
    "    data = pre_processing(corpus)\n",
    "\n",
    "    if type(corpus) == dict:\n",
    "        for concept in data.keys():\n",
    "            for sentence in corpus[concept]:\n",
    "                for token in corpus[concept][sentence]:\n",
    "                    if token != \"'s\":\n",
    "                        words.append(token)\n",
    "\n",
    "    elif type(data) == list:\n",
    "        for element in data:\n",
    "            if element[0] != \"'s\":\n",
    "                words.append(element[0])\n",
    "\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constant', 'human', 'hero', 'obtain', 'clearer', 'person', 'perform', 'student', 'nervous', 'mine', 'block', 'despit', 'easili', 'abil', 'qualiti', 'awe', 'sheet', 'short', 'done', 'drastic', 'persever', 'note', 'restless', 'sharpen', 'emot', 'abnorm', 'multipl', 'compos', 'packag', 'water', 'subject', 'normal', 'cortex', 'equip', 'refin', 'edg', 'scare', 'sever', 'uneas', 'materi', 'usual', 'lightweight', 'inner', 'take', 'caus', 'understand', 'mind', 'allow', 'beyond', 'disturb', 'spiacevol', 'expect', 'handwrit', 'face', 'possibl', 'peopl', 'state', 'lead', 'strength', 'general', 'overcom', 'sharper', 'someon', 'happen', 'inform', 'danger', 'properti', 'withstand', 'bad', 'scari', 'thaht', 'one', 'someth', 'print', 'character', 'blade', 'tool', 'loss', 'negat', 'end', 'rip', 'us', 'make', 'cellulos', 'graphit', 'qualcosa', 'univers', 'creat', 'communic', 'surfac', 'moral', 'sens', 'may', 'avail', 'control', 'action', 'written', 'avoid', 'screen', 'behavior', 'differ', 'consid', 'fiber', 'act', 'tip', 'way', 'provok', 'cut', 'agit', 'discomfort', 'mark', 'unaccommod', 'event', 'ansia', 'context', 'draw', 'particular', 'purpos', 'fine', 'unexpect', 'deriv', 'object', 'characterist', 'felt', 'upset', 'pathway', 'medium', 'write', 'made', 'brutto', 'devic', 'veget', 'ani', 'suspens', 'riski', 'scar', 'go', 'piec', 'laid', 'easi', 'type', 'store', 'wood', 'situat', 'status', 'frighten', 'stationeri', 'worri', 'littl', 'non-relax', 'veri', 'pencil', 'experi', 'kind', 'without', 'high', 'product', 'accada', 'unpleas', 'return', 'use', 'choic', 'tree', 'uncofort', 'abili', 'retriev', 'sharpner', 'abl', 'difficulti', 'understood', 'work', 'anxieti', 'resist', 'mental', 'preoccup', 'craft', 'ventur', 'thin', 'typic', 'difficult', 'flat', 'strang', 'risk', 'fear', 'anticip', 'deal', 'mood', 'thing', 'learn', 'aptitud', 'shapen', 'fold', 'order', 'sharp', 'especi', 'paura', 'futur', 'sad', 'feel', 'smooth']\n"
     ]
    }
   ],
   "source": [
    "print(get_tokens(extract_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restituisce un dictionary con le pseudo-words come chiavi e 0 come valori\n",
    "def generate_pseudo_words(corpus):\n",
    "    list_of_tokens = get_tokens(corpus)\n",
    "    pseudo_words = {}\n",
    "    scores = {}\n",
    "    \n",
    "    for token in range(0, len(list_of_tokens), 2):\n",
    "        try:\n",
    "            pseudo_word = list_of_tokens[token] + list_of_tokens[token + 1]\n",
    "            scores.setdefault(pseudo_word, 0)\n",
    "            pseudo_words.setdefault(pseudo_word, [list_of_tokens[token], list_of_tokens[token + 1]])\n",
    "\n",
    "        except IndexError:\n",
    "            continue\n",
    "        \n",
    "    return [scores, pseudo_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate_pseudo_words(data_extraction('dataset/db.csv'))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restituisce un nuovo corpus con al posto di alcuni token i token con le pseudo-words, e in più restituisce anche una lista associativa con le pseudo-words e il loro score\n",
    "def substitute_pseudo_words(corpus, corpus_to_substitute=None):\n",
    "    corpus = pre_processing(corpus)\n",
    "    corpus_to_substitute = pre_processing(corpus_to_substitute)\n",
    "    new_corpus = {}\n",
    "    pseudo_words_dictionary = generate_pseudo_words(extract_data('dataset/db.csv'))[0]\n",
    "    pseudo_words_map = generate_pseudo_words(extract_data('dataset/db.csv'))[1]\n",
    "\n",
    "\n",
    "    if type(corpus) == dict:\n",
    "\n",
    "        for concept in corpus.keys():\n",
    "            new_corpus.setdefault(concept, {})\n",
    "            \n",
    "            for definition_number in corpus[concept]: \n",
    "\n",
    "                if definition_number not in new_corpus[concept].keys():\n",
    "                    new_corpus[concept].setdefault(definition_number, [])\n",
    "                    \n",
    "                    \n",
    "                for token in corpus[concept][definition_number]:\n",
    "                    keys = list(pseudo_words_map.keys())\n",
    "                    combined = '\\t'.join(keys)\n",
    "                    if token in combined:\n",
    "                        pseudo_word = [pseudo_word for pseudo_word in keys if token in pseudo_word][0]\n",
    "\n",
    "                        # Se la parola la prima parola che compone la pseudo-word è contenuta nella pseudo-word allora si aggiunge 1\n",
    "                        if token in pseudo_words_map[pseudo_word][0]:\n",
    "                            pseudo_words_dictionary[pseudo_word] += 1\n",
    "                        else: \n",
    "                            pseudo_words_dictionary[pseudo_word] -= 1   \n",
    "\n",
    "                        if pseudo_word != '':\n",
    "                            token = token.replace(token, pseudo_word)\n",
    "\n",
    "                    new_corpus[concept][definition_number].append(token)\n",
    "\n",
    "    \n",
    "    \n",
    "    elif type(corpus) == list:\n",
    "        for element in corpus:\n",
    "            token = element[0]\n",
    "            keys = list(pseudo_words_dictionary.keys())\n",
    "            combined = '\\t'.join(keys)\n",
    "            if token in combined:\n",
    "                pseudo_word = [pseudo_word for pseudo_word in keys if token in pseudo_word][0]\n",
    "\n",
    "                # Se la parola la prima parola che compone la pseudo-word è contenuta nella pseudo-word allora si aggiunge 1\n",
    "                try:\n",
    "                    if token in pseudo_words_map[pseudo_word][0]:\n",
    "                        pseudo_words_dictionary[pseudo_word] += 1\n",
    "\n",
    "                except KeyError:\n",
    "                    pseudo_words_dictionary.setdefault(pseudo_word, 1)\n",
    "\n",
    "                try:\n",
    "                    if token in pseudo_words_map[pseudo_word][1]:\n",
    "                        pseudo_words_dictionary[pseudo_word] -= 1\n",
    "\n",
    "                except KeyError:\n",
    "                    pseudo_words_dictionary.setdefault(pseudo_word, -1)\n",
    "\n",
    "                if pseudo_word != '':\n",
    "                    token = token.replace(token, pseudo_word)\n",
    "\n",
    "            for concept in corpus_to_substitute.keys():\n",
    "                if concept not in new_corpus.keys():\n",
    "                    new_corpus.setdefault(concept, {})\n",
    "\n",
    "\n",
    "                for definition_number in corpus_to_substitute[concept]:\n",
    "                    new_definition = []\n",
    "\n",
    "                    # Ogni token viene sostituito con la pseudoword che contiene il token\n",
    "                    for token in corpus_to_substitute[concept][definition_number]:\n",
    "                        if token in pseudo_word:\n",
    "                            #print(f\"{token} - {pseudo_word[:len(token)]}\")\n",
    "\n",
    "                            token = token.replace(token, pseudo_word)\n",
    "\n",
    "                        new_definition.append(token)\n",
    "                    new_corpus[concept].setdefault(definition_number, new_definition)\n",
    "\n",
    "    return [new_corpus, pseudo_words_dictionary]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Courage': {0: ['face',\n",
       "    'situat',\n",
       "    'fear',\n",
       "    'properti',\n",
       "    'allow',\n",
       "    'ani',\n",
       "    'feel',\n",
       "    'despit'],\n",
       "   1: ['abil', 'someth', 'face', 'fear', 'unpleas', 'us', 'make', 'scar'],\n",
       "   2: ['abil', 'without', 'face', 'fear', 'thing'],\n",
       "   4: ['face', 'situat', 'inner', 'particular', 'allow', 'strength', 'thaht'],\n",
       "   5: ['control', 'abil', 'fear'],\n",
       "   6: ['abil', 'someth', 'control', 'fear', 'deal', 'unpleas'],\n",
       "   7: ['abil', 'action', 'avoid', 'take', 'fear', 'riski'],\n",
       "   8: ['abili', 'without', 'action', 'take', 'fear', 'make', 'choic'],\n",
       "   9: ['someth', 'abl', 'fear'],\n",
       "   10: ['abil', 'despit', 'someth', 'frighten'],\n",
       "   11: ['abil', 'someth', 'peopl', 'scare'],\n",
       "   13: ['face', 'us', 'situat', 'consid', 'danger', 'allow', 'feel'],\n",
       "   14: ['scari', 'abil', 'someth', 'may'],\n",
       "   15: ['drastic', 'abil', 'choic', 'make'],\n",
       "   16: ['abil', 'overcom', 'fear'],\n",
       "   17: ['characterist', 'risk', 'person', 'take'],\n",
       "   18: ['qualiti', 'abl', 'danger', 'scare', 'thing', 'general'],\n",
       "   19: ['hero', 'typic', 'behavior'],\n",
       "   20: ['abil', 'face', 'situat', 'difficult'],\n",
       "   21: ['abil', 'someth', 'peopl', 'fear'],\n",
       "   22: ['difficult', 'face', 'situat', 'mind', 'allow', 'strength'],\n",
       "   23: ['abil', \"'s\", 'face', 'fear', 'one'],\n",
       "   24: ['abil', 'block', 'fear'],\n",
       "   25: ['expect', 'someon', 'go', 'emot', 'allow', 'beyond'],\n",
       "   26: ['abil', 'fear', 'danger', 'perform', 'act', 'despit'],\n",
       "   27: ['difficulti',\n",
       "    'fear',\n",
       "    'persever',\n",
       "    'danger',\n",
       "    'withstand',\n",
       "    'strength',\n",
       "    'mental',\n",
       "    'ventur',\n",
       "    'moral'],\n",
       "   28: ['abil', 'situat', 'fear', 'resist', 'scare', 'provok'],\n",
       "   29: ['abil', 'overcom', 'fear'],\n",
       "   30: ['abil', 'difficult', 'face', 'situat', 'fear', 'overcom'],\n",
       "   31: ['human', 'abl', 'scare', 'make', 'aptitud']},\n",
       "  'Paper': {0: ['materi', 'fold', 'written', 'cut', 'cellulos'],\n",
       "   1: ['materi', 'context', 'use', 'deriv', 'sever', 'tree'],\n",
       "   2: ['made', 'cellulos', 'type', 'materi'],\n",
       "   4: ['wood', 'product', 'write', 'obtain', 'use', 'cellulos'],\n",
       "   5: ['flat', 'materi', 'wood', 'write', 'made', 'use'],\n",
       "   6: ['univers',\n",
       "    'short',\n",
       "    'done',\n",
       "    'subject',\n",
       "    'especi',\n",
       "    'write',\n",
       "    'particular',\n",
       "    'student',\n",
       "    'one',\n",
       "    'piec'],\n",
       "   7: ['materi', 'write', 'use', 'deriv', 'tree'],\n",
       "   8: [\"'s\", 'materi', 'cortex', 'obtain', 'tree'],\n",
       "   9: ['materi', 'wood', 'take', 'note', 'use', 'craft'],\n",
       "   10: ['thin', 'write', 'object', 'easili'],\n",
       "   11: ['packag', 'materi', 'print', 'wood', 'write', 'made', 'draw', 'use'],\n",
       "   13: ['product', 'write', 'use', 'general', 'cellulos', 'compos'],\n",
       "   14: ['write', 'surfac'],\n",
       "   15: ['write', 'draw', 'possibl', 'materi'],\n",
       "   16: ['write', 'materi'],\n",
       "   17: ['obtain', 'materi', 'tree'],\n",
       "   18: ['materi', 'wood', 'write', 'obtain', 'use', 'general'],\n",
       "   19: ['written', 'communic', 'medium'],\n",
       "   20: ['write', 'someth'],\n",
       "   21: ['thin', 'made', 'cellulos', 'materi'],\n",
       "   22: ['materi', 'cellulos', 'compos'],\n",
       "   23: ['materi', 'wood', 'purpos', 'obtain', 'use', 'multipl'],\n",
       "   24: ['write', 'use', 'materi'],\n",
       "   25: ['write', 'materi', 'avail'],\n",
       "   26: ['product', \"'s\", 'cellulos', 'tree'],\n",
       "   27: ['sheet',\n",
       "    'usual',\n",
       "    'water',\n",
       "    'screen',\n",
       "    'veget',\n",
       "    'fiber',\n",
       "    'fine',\n",
       "    'suspens',\n",
       "    'felt',\n",
       "    'laid'],\n",
       "   28: ['handwrit', 'use', 'materi', 'print'],\n",
       "   29: ['write', 'materi', 'use'],\n",
       "   30: ['easi', 'materi', 'lightweight', 'write', 'use', 'rip'],\n",
       "   31: ['kind', 'retriev', 'materi', 'store', 'inform', 'one']},\n",
       "  'Apprehension': {0: ['someth',\n",
       "    'strang',\n",
       "    'normal',\n",
       "    'differ',\n",
       "    'caus',\n",
       "    'feel',\n",
       "    'veri',\n",
       "    'abnorm'],\n",
       "   1: ['expect', 'anticip', 'fear'],\n",
       "   2: ['mood', 'agit', 'one', 'feel'],\n",
       "   4: ['state', 'disturb'],\n",
       "   5: ['worri', 'futur'],\n",
       "   6: ['someth', 'understood', 'understand', 'act', 'way'],\n",
       "   7: ['unaccommod', 'event', 'state', 'non-relax', 'mind', 'deriv'],\n",
       "   8: ['sens', 'awe', 'loss', 'fear', 'sad'],\n",
       "   9: ['uncofort',\n",
       "    'someth',\n",
       "    'status',\n",
       "    'situat',\n",
       "    'mental',\n",
       "    'person',\n",
       "    'make',\n",
       "    'feel'],\n",
       "   10: ['mind', 'one', 'frighten', 'state'],\n",
       "   11: ['someth', 'fear', 'happen', 'anxieti', 'bad', 'unexpect'],\n",
       "   13: ['high', 'mental', 'anxieti', 'state'],\n",
       "   14: ['someon', 'feel', 'preoccup', 'someth'],\n",
       "   15: ['fear', 'state', 'person', 'emot', 'feel'],\n",
       "   16: ['happen', 'feel', 'someth', 'bad'],\n",
       "   17: ['person', 'negat', 'emot'],\n",
       "   18: ['character', 'status', 'fear', 'anxieti', 'mental'],\n",
       "   19: ['ansia', 'accada', 'paura', 'brutto', 'qualcosa', 'spiacevol'],\n",
       "   20: ['constant', 'anxieti', 'fear'],\n",
       "   21: ['feel', 'anxieti', 'fear'],\n",
       "   22: ['upset', 'anxieti', 'mind', 'state'],\n",
       "   23: ['mood', 'agit', 'nervous'],\n",
       "   24: ['pathway', 'learn'],\n",
       "   25: ['feel', 'uneas'],\n",
       "   26: ['someth', 'fear', 'happen', 'unpleas', 'anxieti', 'bad'],\n",
       "   27: ['someth', 'fear', 'happen', 'unpleas', 'anxieti', 'bad'],\n",
       "   28: ['discomfort', 'fear', 'feel'],\n",
       "   29: ['anxieti', 'happen', 'unpleas', 'someth'],\n",
       "   30: ['situat', 'fear', 'state', 'particular', 'anxieti'],\n",
       "   31: ['agit', 'restless', 'emot', 'make', 'experi']},\n",
       "  'Sharpener': {0: ['blade',\n",
       "    'tool',\n",
       "    'equip',\n",
       "    'sharpen',\n",
       "    'pencil',\n",
       "    'allow',\n",
       "    'tip'],\n",
       "   1: ['pencil', 'object', 'use', 'shapen'],\n",
       "   2: ['object', 'sharpen', 'pencil'],\n",
       "   4: ['tool', 'use', 'sharpen', 'pencil'],\n",
       "   5: ['littl', 'sharpen', 'pencil', 'allow', 'object'],\n",
       "   6: ['tool', 'someth', 'sharper', 'make'],\n",
       "   7: ['tool', 'use', 'pencil', 'tip', 'cut'],\n",
       "   8: ['tool', 'sharpner', 'mine', 'make'],\n",
       "   9: ['tool', 'pencil', 'sharpen', 'use'],\n",
       "   10: ['object', 'sharpen', 'pencil', 'allow'],\n",
       "   11: ['write', 'sharpen', 'pencil', 'use', 'object'],\n",
       "   13: ['blade', 'tool', 'sharpen', 'pencil', 'use'],\n",
       "   14: ['tool', 'sharpen', 'pencil'],\n",
       "   15: ['tool', 'sharpen', 'pencil'],\n",
       "   16: ['tool', 'sharpen', 'pencil'],\n",
       "   17: ['object', 'sharpen', 'pencil', 'allow'],\n",
       "   18: ['tool', 'pencil', 'sharpen', 'use'],\n",
       "   19: ['someth', 'sharp', 'devic', 'person', 'make'],\n",
       "   20: ['pencil', 'object', 'sharpen', 'use'],\n",
       "   21: ['use', 'object', 'sharpen', 'pencil'],\n",
       "   22: ['stationeri', 'use', 'smooth', 'object', 'graphit'],\n",
       "   23: ['tool', 'use', 'sharpen', 'pencil'],\n",
       "   24: ['sharpen', 'pencil', 'use', 'lead', 'object'],\n",
       "   25: ['order', 'work', 'return', 'pencil', 'object'],\n",
       "   26: ['tool', 'pencil', 'sharp', 'make'],\n",
       "   27: ['someth', 'sharp', 'devic', 'person', 'make'],\n",
       "   28: ['tool', 'creat', 'end', 'refin', 'edg', 'use', 'pencil'],\n",
       "   29: ['tool', 'pencil', 'sharpen', 'use'],\n",
       "   30: ['write', 'mine', 'pencil', 'use', 'object'],\n",
       "   31: ['mark', 'tool', 'clearer', 'use', 'make', 'pencil']}},\n",
       " {'archivedspock': 14,\n",
       "  'kirbypenn': 26,\n",
       "  'tributarybelarusian': 7,\n",
       "  'cemeteryerik': 34,\n",
       "  'csquall': -1,\n",
       "  'kign': -15,\n",
       "  'batsmanseason': 0,\n",
       "  'breedersexede': -3,\n",
       "  'trigfig': -22,\n",
       "  'seanr': 1,\n",
       "  'episodeobverse': 0,\n",
       "  'jessesydney': 10,\n",
       "  'auburndisplaystyle': 1,\n",
       "  'zohoregatta': 7,\n",
       "  'senaself-titled': -1,\n",
       "  'yankeesdiff': 0,\n",
       "  'moverwwe': -19,\n",
       "  'bellanorco': 0,\n",
       "  'cubshelovia': 0,\n",
       "  'spongebobfemdom': 0,\n",
       "  'skorall-star': 1,\n",
       "  'barrrafa': 0,\n",
       "  'floydsesshomaru': 0,\n",
       "  'emanor': -1,\n",
       "  'repriseubf': 0,\n",
       "  'jasontanner': 0,\n",
       "  'divisiónco-production': 1,\n",
       "  'lb3tortola': 0,\n",
       "  'riveraz': 1,\n",
       "  'adirondackluigi': 0,\n",
       "  'bootsorioles': 0,\n",
       "  'tnafacesitting': 0,\n",
       "  'no-dealmorales': 0,\n",
       "  'ïsprite': 0,\n",
       "  'أنwaterford': 0,\n",
       "  'ativansquadron': 0,\n",
       "  'pradabuttigieg': 0,\n",
       "  'railwaybulgarian': 0,\n",
       "  'conductivecfia': 0,\n",
       "  'co-writelonghorn': 0,\n",
       "  'duetbellator': 0,\n",
       "  'allmusicargentine': 0,\n",
       "  'wilmingtonrecords': 0,\n",
       "  'premarinm': 1,\n",
       "  'voivodeshipjosh': 0,\n",
       "  'lieutenantفي': 0,\n",
       "  'costelloastros': 0,\n",
       "  'contribboulevard': 0,\n",
       "  'moreexpand_moretrio': 0,\n",
       "  'riscrunner-up': 0,\n",
       "  'lizicann': 0,\n",
       "  'libpayout': 0,\n",
       "  'bookswebinar': 0,\n",
       "  'maxnatalee': 0,\n",
       "  'barcodecornerback': 0,\n",
       "  'dwarvekino': -1,\n",
       "  'slottelangana': 0,\n",
       "  'seahawkswco': 0,\n",
       "  'treougrave': 0,\n",
       "  'kororton': -1,\n",
       "  'noelupi': 0,\n",
       "  'vininterment': 0,\n",
       "  'u-mrotatable': 0,\n",
       "  'b-sideroster': 0,\n",
       "  'shist': -1,\n",
       "  'donegaldidrex': 0,\n",
       "  'cowboyspalin': 0,\n",
       "  'avasflip-flop': 0,\n",
       "  'pagesdec': 1,\n",
       "  'gulesmodi': 0,\n",
       "  'bearingshampshire': -1,\n",
       "  'vbaronet': -1,\n",
       "  'gscreenplay': -1,\n",
       "  'pngplurality': 0,\n",
       "  'januarysonny': 0,\n",
       "  'mangaquarterback': 0,\n",
       "  'larabeechatham': 0,\n",
       "  'whereinsofia': 0,\n",
       "  'pitchercreek': 0,\n",
       "  'bstarscream': -1,\n",
       "  'ziprecruitereviction': 0,\n",
       "  'néedistancing': 0,\n",
       "  'ipfwemini': 0,\n",
       "  'baronyt': 1,\n",
       "  'kuwabarajakarta': 1,\n",
       "  're-electpatrick': 0,\n",
       "  'shiflettfreecad': 0,\n",
       "  'mariobahrain': 0,\n",
       "  'productionsloyola': 0,\n",
       "  'paylinegit': 0,\n",
       "  'ctvsussex': 0,\n",
       "  'retrieveguwahati': 0,\n",
       "  'manitobadistrict': 0,\n",
       "  'zellfilm': 0,\n",
       "  'hubertsynthroid': 0,\n",
       "  'singulairindicia': 0,\n",
       "  'countyobi-wan': 0,\n",
       "  'pass4surecharlotte': 0,\n",
       "  'glabrouskoyo': 0,\n",
       "  'celinalegolas': 0,\n",
       "  'undefeatedyusuke': 0,\n",
       "  'drummerguitarist': -1,\n",
       "  'shifflettnorthbound': 0,\n",
       "  'kagomehasbro': 0,\n",
       "  'sgmldisambiguation': 0,\n",
       "  'dollyjstor': 0,\n",
       "  'stationkoopa': 0,\n",
       "  'wuhansabha': 0,\n",
       "  'shipyardxena': 0,\n",
       "  'idahocircuitry': 0,\n",
       "  'tagshughesnet': 0,\n",
       "  'flagylcombs': 0,\n",
       "  'repeccaller': 0,\n",
       "  'bushfiremurfreesboro': 0,\n",
       "  'kibaritalin': 1,\n",
       "  'homelessnessigem': 0,\n",
       "  'galwayphp': 0,\n",
       "  'medalnsw': 0,\n",
       "  'wikiino': 1,\n",
       "  'safetylitstarfleet': 0,\n",
       "  'periactinprefecture': 0,\n",
       "  'loisororo': 0,\n",
       "  'spxharv': 0,\n",
       "  'stateroomjd': 0,\n",
       "  'pinyinn': 1,\n",
       "  'yliam': -1,\n",
       "  'albumvenezuela': 0,\n",
       "  'unapadre': 0,\n",
       "  'usrcena': 0,\n",
       "  'cybertronregiment': 0,\n",
       "  'منtribune': 0,\n",
       "  'giulianinadu': 0,\n",
       "  'footballerlondonderry': 0,\n",
       "  'isabelremixe': 0,\n",
       "  'collieryavon': 0,\n",
       "  'thunbergnebraska': 0,\n",
       "  'co-wrotebahamas': 0,\n",
       "  'mickgig': 0,\n",
       "  'comparatoralotof': 0,\n",
       "  'mnaremus': 0,\n",
       "  'inararushden': 0,\n",
       "  'octobermla': 0,\n",
       "  'piroxicamupc': 0,\n",
       "  'hindwingquarantine': 0,\n",
       "  'jriff': -1,\n",
       "  'wellingtonlatinx': 0,\n",
       "  'broadbanddownloads': 1,\n",
       "  'xinjianglimerick': 0,\n",
       "  'ovale': -1,\n",
       "  'blairoptimus': 0,\n",
       "  'lansingwrestler': 0,\n",
       "  'pfuselage': -1,\n",
       "  'emmettespn': 0,\n",
       "  'mccainhms': 0,\n",
       "  'essexbassist': 0,\n",
       "  'great-grandchildbroadcast': 0,\n",
       "  'remakei': 1,\n",
       "  'fayettegaming': 0,\n",
       "  'xpressklamath': 0,\n",
       "  'boiseimpeachment': 0,\n",
       "  'giantsdiethylpropion': 0,\n",
       "  'wildbluemln': 0,\n",
       "  'inflorescencemps': 1,\n",
       "  'bingoromanian': 0,\n",
       "  'dublinalford': 0,\n",
       "  'hreopen': -1,\n",
       "  'blueeaward': 0,\n",
       "  'wichitancaa': 0,\n",
       "  'multiplexertamil': 0,\n",
       "  'decemberwelterweight': 0,\n",
       "  'linebackeró': 1,\n",
       "  'cheneyrahul': 0,\n",
       "  'illegsynopsis': 0,\n",
       "  'songwritercovid': 1,\n",
       "  'zoombruce': 0,\n",
       "  'raiderin-person': 0,\n",
       "  'ephedrinekeanu': 0,\n",
       "  'egraveroyals': 0,\n",
       "  'apparatusbactrim': 0,\n",
       "  'eduardx': 1,\n",
       "  'belgradefilms': 0,\n",
       "  'tipperaryl': 1,\n",
       "  'nathantennessee': 0,\n",
       "  'wunsw': -1,\n",
       "  'colomboseton': 0,\n",
       "  's10inaugural': 0,\n",
       "  'jackpotskyrim': 0,\n",
       "  'ubpaperback': -1,\n",
       "  'pfastemazepam': 0,\n",
       "  'namesmilwaukee': 0,\n",
       "  'codeinebucky': 0,\n",
       "  'decoderjoran': 0,\n",
       "  'bumblebeebettor': 0,\n",
       "  'ufcuwa': 1,\n",
       "  'uconnhanover': 0,\n",
       "  'ezraafc': 0,\n",
       "  'hemorrhoidarts': 0,\n",
       "  'butalbitaljosiah': 0,\n",
       "  'petereeves': 0,\n",
       "  'vflbaseball': 0,\n",
       "  'josékeno': 0,\n",
       "  'doifandom': 0,\n",
       "  'kerryband': 1,\n",
       "  'lionscs1': 0,\n",
       "  'colttamiya': 0,\n",
       "  'saxophonistginny': 0,\n",
       "  'soloobituary': 0,\n",
       "  'rookiemāori': 0,\n",
       "  'kahlocomedy': 0,\n",
       "  'housemateuyghur': 0,\n",
       "  'representativescarrie': 0,\n",
       "  'dodgerslevothyroxine': 0,\n",
       "  'jokerwolverine': 0,\n",
       "  'tenuatenrl': 0,\n",
       "  'irelandltte': 0,\n",
       "  'radionovell': 0,\n",
       "  'guitarpercocet': -1,\n",
       "  'canuckvoldemort': 0,\n",
       "  'adipexkarnataka': 0,\n",
       "  'derekmicroprocessor': 0,\n",
       "  'archivesharlan': 0,\n",
       "  'mayorabilify': 0,\n",
       "  'authorsd': 1,\n",
       "  'dorchesterf': 1,\n",
       "  'warioyeovil': 0,\n",
       "  'wsuwayne': 0,\n",
       "  'عbbc': 0,\n",
       "  'gaelshops': 0,\n",
       "  'irishbronco': 0,\n",
       "  'tournamentpikmin': 0,\n",
       "  'elfuab': -1,\n",
       "  '摘要newchurch': 0,\n",
       "  'wyzantrbi': 0,\n",
       "  'todddrew': 0,\n",
       "  'mikedarren': 0,\n",
       "  'amigabillboard': 0,\n",
       "  'hannityblackjack': 0,\n",
       "  'jpgseptember': 0,\n",
       "  'oclcarchibald': 0,\n",
       "  'premierebiographical': 0,\n",
       "  'contestantnolvadex': 0,\n",
       "  'ficvocal': 0,\n",
       "  'hoodiadoyle': 0,\n",
       "  'texansa': 1,\n",
       "  'welshserie': 0,\n",
       "  'pradeshoblast': 0,\n",
       "  'agravegaa': 0,\n",
       "  'songembodiment': -1,\n",
       "  'junebaccarat': 0,\n",
       "  're-electiontoys': 0,\n",
       "  'uweblog': -1,\n",
       "  'batmanwesnoth': 0,\n",
       "  'agnesissn': 0,\n",
       "  'playofffag': 0,\n",
       "  'hinatagore': 0,\n",
       "  'kentuckybelfast': 0,\n",
       "  'nashvillehydrocodone': 0,\n",
       "  'pac-12winning': 0,\n",
       "  'first-classalex': 0,\n",
       "  'rivotrilchristchurch': 0,\n",
       "  'kennykib': 1,\n",
       "  'strikeoutemail': 0}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substitute_pseudo_words(extract_data('dataset/db.csv'), extract_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score to probability\n",
    "def score_converter(score, scores_dictionary):\n",
    "    temp = []\n",
    "    rate = 0.00\n",
    "\n",
    "    for value in scores_dictionary.values():\n",
    "        temp.append(value)\n",
    "    \n",
    "    pos_max_score = max(temp)\n",
    "    neg_max_score = min(temp)\n",
    "\n",
    "    # Percentuale di quanto lo score è positivo (1.0 se è uguale al massimo)\n",
    "    if score > 0:\n",
    "        rate = score / pos_max_score\n",
    "\n",
    "    elif score < 0:\n",
    "        # Percentuale di quanto lo score è negativo (1.0 se è uguale al massimo)\n",
    "        rate = score / neg_max_score\n",
    "        \n",
    "    rate = round(rate, 2)\n",
    "    return rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_converter(-10, substitute_pseudo_words(extract_data('dataset/db.csv'), extract_data())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsi(target_corpus, scores, pseudo_words):\n",
    "\n",
    "    for concept in target_corpus.keys():\n",
    "        for index in target_corpus[concept]:\n",
    "            for token_index in range(len(target_corpus[concept][index])):\n",
    "                token = target_corpus[concept][index][token_index]\n",
    "\n",
    "                if token in scores.keys():\n",
    "                    pseudo_word = pseudo_words[token]\n",
    "\n",
    "                    score = scores[token]\n",
    "\n",
    "                    if score > 0:\n",
    "                        target_corpus[concept][index][token_index] = token.replace(token, pseudo_word[0]) \n",
    "\n",
    "                    elif score == 0:\n",
    "                        random_index = random.randint(0, 1)\n",
    "                        target_corpus[concept][index][token_index] = token.replace(token, pseudo_word[random_index]) \n",
    "\n",
    "                    else: \n",
    "                        target_corpus[concept][index][token_index] = token.replace(token, pseudo_word[1]) \n",
    "\n",
    "    return target_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsi2(target_corpus, scores, pseudo_words):\n",
    "\n",
    "    for concept in target_corpus.keys():\n",
    "        for index in target_corpus[concept]:\n",
    "            for token_index in range(len(target_corpus[concept][index])):\n",
    "                token = target_corpus[concept][index][token_index]\n",
    "\n",
    "                if token in scores.keys():\n",
    "                    pseudo_word = pseudo_words[token]\n",
    "\n",
    "                    score = scores[token]\n",
    "\n",
    "                    rate = score_converter(score, substitute_pseudo_words(extract_data())[1])\n",
    "                    rnd = random.randint(0.00, 1.00)\n",
    "\n",
    "                    if score > 0:\n",
    "                        if rate > rnd:\n",
    "                            target_corpus[concept][index][token_index] = token.replace(token, pseudo_word[0]) \n",
    "                        else:\n",
    "                            target_corpus[concept][index][token_index] = token.replace(token, pseudo_word[1]) \n",
    "\n",
    "                    elif score == 0:\n",
    "                        random_index = random.randint(0, 1)\n",
    "                        target_corpus[concept][index][token_index] = token.replace(token, pseudo_word[random_index]) \n",
    "\n",
    "                    else: \n",
    "                        if rate > rnd:\n",
    "                            target_corpus[concept][index][token_index] = token.replace(token, pseudo_word[0]) \n",
    "                        else:\n",
    "                            target_corpus[concept][index][token_index] = token.replace(token, pseudo_word[1]) \n",
    "\n",
    "    return target_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ac(wsi_corpus, real_corpus):\n",
    "    real_corpus = pre_processing(real_corpus)\n",
    "    right_previsions = 0\n",
    "    total = 0\n",
    "    for concept in real_corpus.keys():\n",
    "        for index in real_corpus[concept]:\n",
    "            for token_index in range(len(real_corpus[concept][index])):\n",
    "                token = real_corpus[concept][index][token_index]\n",
    "                \n",
    "                if token == wsi_corpus[concept][index][token_index]:\n",
    "                    right_previsions += 1\n",
    "                    \n",
    "                total += 1\n",
    "    accuracy = right_previsions / total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "input_corpus = substitute_pseudo_words(extract_data())[0]\n",
    "\n",
    "scores = substitute_pseudo_words(extract_data('dataset/db.csv'), extract_data())[1]\n",
    "\n",
    "pseudo_words = generate_pseudo_words(extract_data('dataset/db.csv'))[1]\n",
    "\n",
    "wsi_corpus = wsi2(input_corpus, scores, pseudo_words)\n",
    "\n",
    "accuracy = eval_ac(wsi_corpus, extract_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy, 2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
