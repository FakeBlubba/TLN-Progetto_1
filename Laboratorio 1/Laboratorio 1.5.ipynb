{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 1.5\n",
    "Non ciclare su tutti i synset, ma sfruttare il meccanismo del \"genus\": parte dal presupposto di localizzare un concetto a partire dal suo iperonimo per poi aggiungere propriet√† che lo caratterizzano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn, brown, stopwords\n",
    "from nltk import word_tokenize, SnowballStemmer\n",
    "import spacy #pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz # CONTROLLA VERSIONE CHE SERVE LA 3.0/3.1.0!\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# Crea la variabile stop_words con le stop word e con la punteggiatura\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(set(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(path='dataset/defs.csv'):\n",
    "    if path == 'dataset/defs.csv':\n",
    "        df = pd.read_csv(path, header=0)\n",
    "        df = df.dropna()\n",
    "        df.drop(['Partecipante'],axis=1,inplace=True)\n",
    "        data = df.to_dict()\n",
    "\n",
    "    elif 'dataset/db.csv':\n",
    "        df = pd.read_csv(path, header=0, sep=',')\n",
    "        data = df.values.tolist()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_indexes(data):\n",
    "    defs = {}\n",
    "\n",
    "    for concept in data.keys():\n",
    "        i = 0\n",
    "        defs.setdefault(concept,{})\n",
    "        for index in data[concept]:\n",
    "            defs[concept].setdefault(i, data[concept][index])\n",
    "            i += 1\n",
    "    return defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "    for names in data.keys():\n",
    "        for index in data[names]:\n",
    "            data[names][index] = word_tokenize(data[names][index].lower())\n",
    "\n",
    "    for names in data.keys():\n",
    "        for index in data[names]:\n",
    "            temp = []\n",
    "            for token in data[names][index]:\n",
    "                if token not in stop_words:\n",
    "                    temp.append(token)\n",
    "            data[names][index] = temp\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Courage': {0: ['property', 'allows', 'face', 'situation', 'despite', 'feeling', 'fear'], 1: ['ability', 'face', 'fears', 'something', 'scars', 'us', 'makes', 'us', 'unpleasent'], 2: ['ability', 'face', 'thing', 'without', 'fear'], 3: ['inner', 'strength', 'thaht', 'allow', 'face', 'particular', 'situations'], 4: ['ability', 'control', 'fear'], 5: ['ability', 'control', 'fear', 'willing', 'deal', 'something', 'unpleasant'], 6: ['ability', 'avoid', 'fear', 'take', 'risky', 'actions'], 7: ['abiliity', 'make', 'choices', 'take', 'action', 'without', 'fear'], 8: ['able', 'something', 'fearful'], 9: ['ability', 'something', 'despite', 'frightened'], 10: ['ability', 'something', 'scares', 'people'], 11: ['feeling', 'allows', 'us', 'face', 'situations', 'considered', 'dangerous'], 12: ['ability', 'something', 'may', 'scary'], 13: ['ability', 'make', 'drastic', 'choices'], 14: ['ability', 'overcome', 'fear'], 15: ['characteristic', 'person', 'taking', 'risk'], 16: ['quality', 'able', 'things', 'generally', 'dangerous', 'scaring'], 17: ['behavior', 'typical', 'hero'], 18: ['ability', 'face', 'difficult', 'situations'], 19: ['ability', 'something', 'people', 'fear'], 20: ['strength', 'mind', 'allows', 'face', 'difficult', 'situations'], 21: ['ability', 'face', 'one', \"'s\", 'fears'], 22: ['ability', 'blocked', 'fear'], 23: ['emotion', 'allows', 'someone', 'go', 'beyond', 'expectations'], 24: ['ability', 'perform', 'dangerous', 'act', 'despite', 'fear'], 25: ['mental', 'moral', 'strength', 'venture', 'persevere', 'withstand', 'danger', 'fear', 'difficulty'], 26: ['ability', 'resist', 'fear', 'scared', 'situations', 'provoke', 'fear'], 27: ['ability', 'overcome', 'fears'], 28: ['ability', 'overcome', 'fear', 'face', 'difficult', 'situations'], 29: ['aptitude', 'human', 'makes', 'able', 'scared']}, 'Paper': {0: ['cellulose', 'material', 'cut', 'folded', 'written'], 1: ['material', 'derived', 'trees', 'used', 'several', 'context'], 2: ['type', 'material', 'made', 'cellulose'], 3: ['product', 'obtained', 'wood', 'cellulose', 'used', 'writing'], 4: ['flat', 'material', 'made', 'wood', 'used', 'writing'], 5: ['short', 'piece', 'writing', 'particular', 'subject', 'especially', 'one', 'done', 'university', 'students'], 6: ['material', 'derived', 'trees', 'used', 'write'], 7: ['material', 'obtained', 'trees', \"'s\", 'cortex'], 8: ['material', 'crafted', 'wood', 'used', 'taking', 'notes'], 9: ['thin', 'object', 'easily', 'write'], 10: ['material', 'made', 'wood', 'materials', 'used', 'writing', 'drawing', 'printing', 'packaging', 'material'], 11: ['product', 'composed', 'cellulose', 'generally', 'used', 'writing'], 12: ['surface', 'write'], 13: ['material', 'possible', 'write', 'draw'], 14: ['material', 'write'], 15: ['material', 'obtained', 'trees'], 16: ['material', 'obtained', 'wood', 'used', 'generally', 'write'], 17: ['medium', 'written', 'communication'], 18: ['something', 'write'], 19: ['thin', 'material', 'made', 'cellulose'], 20: ['material', 'composed', 'cellulose'], 21: ['material', 'obtained', 'wood', 'used', 'multiple', 'purposes'], 22: ['material', 'used', 'writing'], 23: ['material', 'available', 'writing'], 24: ['product', 'tree', \"'s\", 'cellulose'], 25: ['felted', 'sheet', 'usually', 'vegetable', 'fibers', 'laid', 'fine', 'screen', 'water', 'suspension'], 26: ['material', 'used', 'handwriting', 'printing'], 27: ['material', 'used', 'writing'], 28: ['lightweight', 'material', 'used', 'write', 'easy', 'rip'], 29: ['kind', 'material', 'one', 'store', 'retrieve', 'information']}, 'Apprehension': {0: ['something', 'strange', 'causes', 'strange', 'feeling', 'strangeness', 'different', 'normal', 'abnormal'], 1: ['fearful', 'expectation', 'anticipation'], 2: ['moode', 'one', 'feel', 'agitation'], 3: ['state', 'disturbance'], 4: ['worry', 'future'], 5: ['act', 'understanding', 'something', 'way', 'something', 'understood'], 6: ['non-relaxed', 'state', 'mind', 'derived', 'unaccommodating', 'events'], 7: ['sense', 'loss', 'sadness', 'fear', 'awe'], 8: ['mental', 'status', 'make', 'person', 'feel', 'uncofortable', 'something', 'situation'], 9: ['state', 'mind', 'one', 'frightened'], 10: ['anxiety', 'fear', 'something', 'bad', 'unexpected', 'happen'], 11: ['mental', 'state', 'high', 'anxiety'], 12: ['feeling', 'preoccupation', 'something', 'someone'], 13: ['emotional', 'state', 'person', 'feels', 'fear'], 14: ['feeling', 'something', 'bad', 'happen'], 15: ['negative', 'emotion', 'person'], 16: ['mental', 'status', 'characterized', 'anxiety', 'fear'], 17: ['ansia', 'paura', 'che', 'accada', 'qualcosa', 'di', 'brutto', 'spiacevole'], 18: ['constant', 'fear', 'anxiety'], 19: ['feeling', 'fear', 'anxiety'], 20: ['state', 'mind', 'upset', 'anxiety'], 21: ['mood', 'agitation', 'nervousness'], 22: ['pathway', 'learning'], 23: ['feeling', 'unease'], 24: ['anxiety', 'fear', 'something', 'bad', 'unpleasant', 'happen'], 25: ['anxiety', 'fear', 'something', 'bad', 'unpleasant', 'happen'], 26: ['feeling', 'fear', 'discomfort'], 27: ['anxiety', 'something', 'unpleasant', 'happen'], 28: ['state', 'anxiety', 'fear', 'particular', 'situation'], 29: ['emotion', 'makes', 'experiences', 'restless', 'agitated']}, 'Sharpener': {0: ['tool', 'equipped', 'blade', 'allows', 'sharpen', 'tip', 'pencils'], 1: ['object', 'used', 'shapen', 'pencil'], 2: ['object', 'sharpen', 'pencil'], 3: ['tool', 'used', 'sharpen', 'pencils'], 4: ['little', 'object', 'allow', 'sharpen', 'pencil'], 5: ['tool', 'making', 'something', 'sharper'], 6: ['tool', 'used', 'cut', 'pencil', 'tip'], 7: ['tool', 'making', 'mine', 'sharpner'], 8: ['tool', 'used', 'sharpen', 'pencil'], 9: ['object', 'allows', 'sharpen', 'pencil'], 10: ['object', 'used', 'sharpen', 'pencils', 'write'], 11: ['tool', 'blade', 'used', 'sharpen', 'pencils'], 12: ['tool', 'sharpening', 'pencil'], 13: ['tool', 'sharpen', 'pencil'], 14: ['tool', 'pencil', 'sharpening'], 15: ['object', 'allows', 'sharpen', 'pencil'], 16: ['tool', 'used', 'sharpen', 'pencils'], 17: ['person', 'device', 'makes', 'something', 'sharp'], 18: ['object', 'use', 'sharpen', 'pencils'], 19: ['object', 'used', 'sharpen', 'pencils'], 20: ['stationery', 'object', 'used', 'smooth', 'graphite'], 21: ['tool', 'used', 'sharpen', 'pencils'], 22: ['object', 'used', 'sharpen', 'lead', 'pencils'], 23: ['object', 'returning', 'pencil', 'working', 'order'], 24: ['sharp', 'tool', 'make', 'pencil', 'sharp'], 25: ['person', 'device', 'makes', 'something', 'sharp'], 26: ['tool', 'used', 'creating', 'refined', 'edge', 'end', 'pencil'], 27: ['tool', 'used', 'sharpen', 'pencil'], 28: ['object', 'used', 'use', 'mine', 'pencil', 'write'], 29: ['tool', 'used', 'make', 'mark', 'pencil', 'clearer']}}\n"
     ]
    }
   ],
   "source": [
    "# Estrazione dei dati dei concetti in un dictionary\n",
    "data = sort_indexes(extract_corpus())\n",
    "\n",
    "# Pre-processing dell'input\n",
    "data = pre_processing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_most_frequent_words(definitions, keep=10):\n",
    "    frequent_words = {}\n",
    "    temp = []\n",
    "    for definition_index in definitions.keys():\n",
    "\n",
    "        for word in definitions[definition_index]:\n",
    "\n",
    "            if word not in frequent_words:\n",
    "                frequent_words.setdefault(word, [0] * len(definitions.values()))\n",
    "                frequent_words[word][definition_index] += 1\n",
    "            \n",
    "            else: \n",
    "                frequent_words[word][definition_index] += 1\n",
    "\n",
    "\n",
    "    for word in frequent_words.keys():\n",
    "        temp.append([word, sum(frequent_words[word])])\n",
    "\n",
    "\n",
    "    # Le parole vengono aggiunte all'interno del dictionary e vengono riordinate\n",
    "    temp = sorted(temp, key=itemgetter(1), reverse=True)\n",
    "\n",
    "    del frequent_words\n",
    "    frequent_words = {}\n",
    "\n",
    "    i = 0\n",
    "    for tpl in temp: \n",
    "        if i < keep:\n",
    "            frequent_words.setdefault(tpl[0], tpl[1])\n",
    "            i += 1\n",
    "    \n",
    "    return frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'something': 10, 'fear': 10, 'anxiety': 10, 'state': 7, 'feeling': 6, 'happen': 5, 'bad': 4, 'mind': 3, 'mental': 3, 'person': 3}\n"
     ]
    }
   ],
   "source": [
    "freq = generate_most_frequent_words(data['Apprehension'])\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synsets': [Synset('condition.n.01')], 'matches': [3]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_corpus(corpus, concept, synsets):\n",
    "    if synsets != []:\n",
    "        winners = {}\n",
    "        winners.setdefault('synsets', [])\n",
    "        winners.setdefault('matches', [])\n",
    "\n",
    "        max_overlap = -1\n",
    "        senses = []\n",
    "        matches = []\n",
    "        sentences = set()\n",
    "\n",
    "        for synset in synsets:\n",
    "            signature = []\n",
    "            definition = set(synset.definition().split())\n",
    "            examples = synset.examples()\n",
    "\n",
    "            for example in examples:\n",
    "                definition.update(example.split())\n",
    "\n",
    "\n",
    "            for index in corpus[concept]:\n",
    "                sentence = set(corpus[concept][index])\n",
    "                sentences.update(sentence)\n",
    "            overlap = len(sentences & definition)\n",
    "                #print(overlap)\n",
    "\n",
    "            # La definizione con pi√π overlapping sar√† l'output\n",
    "            if overlap > max_overlap:\n",
    "                senses.append(synset)\n",
    "                matches.append(overlap)\n",
    "                max_overlap = overlap\n",
    "\n",
    "        winners = {'synsets': senses, 'matches': matches}\n",
    "\n",
    "        return winners # 'Synset_vincitori': [lista_di_synset], 'numero di match': n\n",
    "\n",
    "    else: \n",
    "        print(synsets)\n",
    "        winners = {'synsets': ['error'], 'matches': [-1]}\n",
    "        \n",
    "        return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = wn.synsets('condition')\n",
    "print(evaluate_corpus(data, 'Apprehension', test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypernyms(concept, max_depth=2, corpus=data):\n",
    "    temp = {}\n",
    "    winners = {}\n",
    "    hyponyms = []\n",
    "    words = generate_most_frequent_words(corpus[concept])\n",
    "    \n",
    "    for word in words.keys():\n",
    "        temp.setdefault(word, wn.synsets(word))\n",
    "    \n",
    "    depth = 0\n",
    "\n",
    "    while depth < max_depth:\n",
    "\n",
    "        for word in words.keys():\n",
    "            hyponyms = []\n",
    "            for synset_id in range(len(temp[word])):\n",
    "                syn = [temp[word][synset_id]]\n",
    "                temp[word][synset_id] = []\n",
    "                \n",
    "                # Se vi √® un solo synset allora questo diventa una lista di iponimi\n",
    "                for element in syn:\n",
    "                    if type(element) == list:\n",
    "                        \n",
    "                        synsets_list = element[0]\n",
    "\n",
    "                        for element in synsets_list:\n",
    "                            if element.hyponyms() != []:\n",
    "                                for hyponym in element.hyponyms():\n",
    "                                    hyponyms.append(hyponym)\n",
    "\n",
    "                    elif element.hyponyms() != []:\n",
    "                        for hyponym in element.hyponyms():\n",
    "                            hyponyms.append(hyponym)\n",
    "                    \n",
    "                temp[word][synset_id].append(hyponyms)\n",
    "            \n",
    "            if hyponyms == []:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                winners.setdefault(depth, evaluate_corpus(corpus, concept, hyponyms))\n",
    "            \n",
    "\n",
    "        depth += 1\n",
    "    return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'synsets': [Synset('alarm.n.01'), Synset('apprehension.n.01')], 'matches': [1, 3]}, 1: {'synsets': [Synset('chill.n.04'), Synset('foreboding.n.01')], 'matches': [0, 2]}, 2: {'synsets': [Synset('presage.n.01'), Synset('hesitance.n.01')], 'matches': [1, 2]}, 3: {'synsets': [Synset('fact.n.03'), Synset('insecureness.n.01'), Synset('pass.n.08'), Synset('disturbance.n.02')], 'matches': [1, 2, 3, 4]}}\n"
     ]
    }
   ],
   "source": [
    "print(generate_hypernyms('Apprehension', max_depth=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_genus(corpus=data, depth=2, keep=10):\n",
    "    genus = {}\n",
    "\n",
    "    for concept in corpus.keys():\n",
    "        words = generate_most_frequent_words(corpus[concept], keep=keep)\n",
    "        genus.setdefault(concept, generate_hypernyms(concept, max_depth=depth))\n",
    "\n",
    "    for concept in genus.keys():\n",
    "        max_score = -1\n",
    "        for i in genus[concept]:\n",
    "            synsets = genus[concept][i]['synsets']\n",
    "            score = sum(genus[concept][i]['matches'])\n",
    "            \n",
    "            if genus[concept][i]['matches'] == []:\n",
    "                pass\n",
    "\n",
    "            if score > max_score:\n",
    "                final_synsets = synsets\n",
    "                max_score = score\n",
    "\n",
    "        genus[concept] = final_synsets\n",
    "\n",
    "    return genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Courage': [Synset('adaptability.n.01'), Synset('form.n.14'), Synset('penetration.n.04')], 'Paper': [Synset('carborundum.n.01'), Synset('steel_wool.n.01'), Synset('binder.n.02'), Synset('carbon_paper.n.01'), Synset('writing_paper.n.01')], 'Apprehension': [Synset('alarm.n.01'), Synset('apprehension.n.01')], 'Sharpener': [Synset('abrading_stone.n.01'), Synset('blade.n.09'), Synset('dibble.n.01'), Synset('wire_stripper.n.01')]}\n"
     ]
    }
   ],
   "source": [
    "print(generate_genus(depth=3, keep=5))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
